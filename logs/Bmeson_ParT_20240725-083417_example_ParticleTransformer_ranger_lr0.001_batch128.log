[2024-07-25 08:34:17,235] INFO: args:
 - ('regression_mode', True)
 - ('data_config', 'data/Bmeson/bm_kin.yaml')
 - ('extra_selection', None)
 - ('extra_test_selection', None)
 - ('data_train', ['data/Bmeson/train_file.parquet'])
 - ('data_val', [])
 - ('data_test', ['data/Bmeson/test_file.parquet'])
 - ('data_fraction', 1)
 - ('file_fraction', 1)
 - ('fetch_by_files', False)
 - ('fetch_step', 1.0)
 - ('in_memory', True)
 - ('train_val_split', 0.8889)
 - ('no_remake_weights', False)
 - ('demo', False)
 - ('lr_finder', None)
 - ('tensorboard', 'Bmeson_kin_ParT')
 - ('tensorboard_custom_fn', None)
 - ('network_config', 'networks/example_ParticleTransformer.py')
 - ('network_option', [])
 - ('model_prefix', 'training/Bmeson/ParT/20240725-083417_example_ParticleTransformer_ranger_lr0.001_batch128/net')
 - ('load_model_weights', None)
 - ('exclude_model_weights', None)
 - ('freeze_model_weights', None)
 - ('num_epochs', 1)
 - ('steps_per_epoch', 12500)
 - ('steps_per_epoch_val', 1562)
 - ('samples_per_epoch', 1600000)
 - ('samples_per_epoch_val', 200000)
 - ('optimizer', 'ranger')
 - ('optimizer_option', [['weight_decay', '0.01']])
 - ('lr_scheduler', 'flat+decay')
 - ('warmup_steps', 0)
 - ('load_epoch', None)
 - ('start_lr', 0.001)
 - ('batch_size', 128)
 - ('use_amp', True)
 - ('gpus', '0')
 - ('predict_gpus', '0')
 - ('num_workers', 1)
 - ('predict', False)
 - ('predict_output', 'pred.root')
 - ('export_onnx', None)
 - ('onnx_opset', 15)
 - ('io_test', False)
 - ('copy_inputs', False)
 - ('log', 'logs/Bmeson_ParT_20240725-083417_example_ParticleTransformer_ranger_lr0.001_batch128.log')
 - ('print', False)
 - ('profile', False)
 - ('backend', None)
 - ('cross_validation', None)
 - ('_auto_model_name', '20240725-083417_example_ParticleTransformer_ranger_lr0.001_batch128')
 - ('local_rank', None)
[2024-07-25 08:34:17,235] INFO: Running in regression mode
[2024-07-25 08:34:17,343] INFO: Using 1 files for training, range: (0, 0.8889)
[2024-07-25 08:34:17,343] INFO: Using 1 files for validation, range: (0.8889, 1)
[2024-07-25 08:34:17,347] INFO: [0;37mpreprocess config: {'method': 'manual', 'data_fraction': 0.5, 'params': None}[0m
[2024-07-25 08:34:17,347] INFO: [0;37mselection: None[0m
[2024-07-25 08:34:17,347] INFO: [0;37mtest_time_selection: None[0m
[2024-07-25 08:34:17,347] INFO: [0;37mvar_funcs:
 - ('part_mask', 'ak.ones_like(part_px)')
 - ('jet_isQ', 'label')
 - ('jet_isG', '1-label')
 - ('mass', 'label')
 - ('truth_label', 'mass')[0m
[2024-07-25 08:34:17,347] INFO: [0;37minput_names: ('pf_points', 'pf_features', 'pf_vectors', 'pf_mask')[0m
[2024-07-25 08:34:17,347] INFO: [0;37minput_dicts:
 - ('pf_points', ['part_px', 'part_py', 'part_pz'])
 - ('pf_features', ['part_px', 'part_py', 'part_pz', 'part_energy', 'part_charge'])
 - ('pf_vectors', ['part_px', 'part_py', 'part_pz', 'part_energy'])
 - ('pf_mask', ['part_mask'])[0m
[2024-07-25 08:34:17,347] INFO: [0;37minput_shapes:
 - ('pf_points', (-1, 3, 128))
 - ('pf_features', (-1, 5, 128))
 - ('pf_vectors', (-1, 4, 128))
 - ('pf_mask', (-1, 1, 128))[0m
[2024-07-25 08:34:17,347] INFO: [0;37mpreprocess_params:
 - ('part_px', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_py', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_pz', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_energy', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_charge', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_mask', {'length': 128, 'pad_mode': 'constant', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})[0m
[2024-07-25 08:34:17,347] INFO: [0;37mlabel_names: ('truth_label',)[0m
[2024-07-25 08:34:17,347] INFO: [0;37mobserver_names: ()[0m
[2024-07-25 08:34:17,347] INFO: [0;37mmonitor_variables: ()[0m
[2024-07-25 08:34:17,350] INFO: [0;37mpreprocess config: {'method': 'manual', 'data_fraction': 0.5, 'params': None}[0m
[2024-07-25 08:34:17,350] INFO: [0;37mselection: None[0m
[2024-07-25 08:34:17,351] INFO: [0;37mtest_time_selection: None[0m
[2024-07-25 08:34:17,351] INFO: [0;37mvar_funcs:
 - ('part_mask', 'ak.ones_like(part_px)')
 - ('jet_isQ', 'label')
 - ('jet_isG', '1-label')
 - ('mass', 'label')
 - ('truth_label', 'mass')[0m
[2024-07-25 08:34:17,351] INFO: [0;37minput_names: ('pf_points', 'pf_features', 'pf_vectors', 'pf_mask')[0m
[2024-07-25 08:34:17,351] INFO: [0;37minput_dicts:
 - ('pf_points', ['part_px', 'part_py', 'part_pz'])
 - ('pf_features', ['part_px', 'part_py', 'part_pz', 'part_energy', 'part_charge'])
 - ('pf_vectors', ['part_px', 'part_py', 'part_pz', 'part_energy'])
 - ('pf_mask', ['part_mask'])[0m
[2024-07-25 08:34:17,351] INFO: [0;37minput_shapes:
 - ('pf_points', (-1, 3, 128))
 - ('pf_features', (-1, 5, 128))
 - ('pf_vectors', (-1, 4, 128))
 - ('pf_mask', (-1, 1, 128))[0m
[2024-07-25 08:34:17,351] INFO: [0;37mpreprocess_params:
 - ('part_px', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_py', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_pz', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_energy', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_charge', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_mask', {'length': 128, 'pad_mode': 'constant', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})[0m
[2024-07-25 08:34:17,351] INFO: [0;37mlabel_names: ('truth_label',)[0m
[2024-07-25 08:34:17,351] INFO: [0;37mobserver_names: ()[0m
[2024-07-25 08:34:17,351] INFO: [0;37mmonitor_variables: ()[0m
[2024-07-25 08:34:17,354] INFO: [0;37mpreprocess config: {'method': 'manual', 'data_fraction': 0.5, 'params': None}[0m
[2024-07-25 08:34:17,354] INFO: [0;37mselection: None[0m
[2024-07-25 08:34:17,354] INFO: [0;37mtest_time_selection: None[0m
[2024-07-25 08:34:17,354] INFO: [0;37mvar_funcs:
 - ('part_mask', 'ak.ones_like(part_px)')
 - ('jet_isQ', 'label')
 - ('jet_isG', '1-label')
 - ('mass', 'label')
 - ('truth_label', 'mass')[0m
[2024-07-25 08:34:17,354] INFO: [0;37minput_names: ('pf_points', 'pf_features', 'pf_vectors', 'pf_mask')[0m
[2024-07-25 08:34:17,354] INFO: [0;37minput_dicts:
 - ('pf_points', ['part_px', 'part_py', 'part_pz'])
 - ('pf_features', ['part_px', 'part_py', 'part_pz', 'part_energy', 'part_charge'])
 - ('pf_vectors', ['part_px', 'part_py', 'part_pz', 'part_energy'])
 - ('pf_mask', ['part_mask'])[0m
[2024-07-25 08:34:17,354] INFO: [0;37minput_shapes:
 - ('pf_points', (-1, 3, 128))
 - ('pf_features', (-1, 5, 128))
 - ('pf_vectors', (-1, 4, 128))
 - ('pf_mask', (-1, 1, 128))[0m
[2024-07-25 08:34:17,354] INFO: [0;37mpreprocess_params:
 - ('part_px', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_py', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_pz', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_energy', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_charge', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_mask', {'length': 128, 'pad_mode': 'constant', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})[0m
[2024-07-25 08:34:17,354] INFO: [0;37mlabel_names: ('truth_label',)[0m
[2024-07-25 08:34:17,354] INFO: [0;37mobserver_names: ()[0m
[2024-07-25 08:34:17,354] INFO: [0;37mmonitor_variables: ()[0m
[2024-07-25 08:34:17,357] INFO: [0;37mpreprocess config: {'method': 'manual', 'data_fraction': 0.5, 'params': None}[0m
[2024-07-25 08:34:17,357] INFO: [0;37mselection: None[0m
[2024-07-25 08:34:17,358] INFO: [0;37mtest_time_selection: None[0m
[2024-07-25 08:34:17,358] INFO: [0;37mvar_funcs:
 - ('part_mask', 'ak.ones_like(part_px)')
 - ('jet_isQ', 'label')
 - ('jet_isG', '1-label')
 - ('mass', 'label')
 - ('truth_label', 'mass')[0m
[2024-07-25 08:34:17,358] INFO: [0;37minput_names: ('pf_points', 'pf_features', 'pf_vectors', 'pf_mask')[0m
[2024-07-25 08:34:17,358] INFO: [0;37minput_dicts:
 - ('pf_points', ['part_px', 'part_py', 'part_pz'])
 - ('pf_features', ['part_px', 'part_py', 'part_pz', 'part_energy', 'part_charge'])
 - ('pf_vectors', ['part_px', 'part_py', 'part_pz', 'part_energy'])
 - ('pf_mask', ['part_mask'])[0m
[2024-07-25 08:34:17,358] INFO: [0;37minput_shapes:
 - ('pf_points', (-1, 3, 128))
 - ('pf_features', (-1, 5, 128))
 - ('pf_vectors', (-1, 4, 128))
 - ('pf_mask', (-1, 1, 128))[0m
[2024-07-25 08:34:17,358] INFO: [0;37mpreprocess_params:
 - ('part_px', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_py', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_pz', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_energy', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_charge', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_mask', {'length': 128, 'pad_mode': 'constant', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})[0m
[2024-07-25 08:34:17,358] INFO: [0;37mlabel_names: ('truth_label',)[0m
[2024-07-25 08:34:17,358] INFO: [0;37mobserver_names: ()[0m
[2024-07-25 08:34:17,358] INFO: [0;37mmonitor_variables: ()[0m
[2024-07-25 08:34:17,362] INFO: Network options: {}
[2024-07-25 08:34:17,362] INFO: Model config: {'input_dim': 5, 'num_classes': 1, 'pair_input_dim': 4, 'use_pre_activation_pair': False, 'embed_dims': [128, 512, 128], 'pair_embed_dims': [64, 64, 64], 'num_heads': 8, 'num_layers': 8, 'num_cls_layers': 2, 'block_params': None, 'cls_block_params': {'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0}, 'fc_params': [], 'activation': 'relu', 'trim': True, 'for_inference': False, 'use_amp': True}
[2024-07-25 08:34:17,362] INFO: cfg_block: {'embed_dim': 128, 'num_heads': 8, 'ffn_ratio': 4, 'dropout': 0.1, 'attn_dropout': 0.1, 'activation_dropout': 0.1, 'add_bias_kv': False, 'activation': 'relu', 'scale_fc': True, 'scale_attn': True, 'scale_heads': True, 'scale_resids': True}
[2024-07-25 08:34:17,362] INFO: cfg_cls_block: {'embed_dim': 128, 'num_heads': 8, 'ffn_ratio': 4, 'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0, 'add_bias_kv': False, 'activation': 'relu', 'scale_fc': True, 'scale_attn': True, 'scale_heads': True, 'scale_resids': True}
[2024-07-25 08:34:19,015] INFO: [0;37mWarning: module SequenceTrimmer is treated as a zero-op.[0m
[2024-07-25 08:34:19,015] INFO: [0;37mWarning: module LayerNorm is treated as a zero-op.[0m
[2024-07-25 08:34:19,015] INFO: [0;37mWarning: module Embed is treated as a zero-op.[0m
[2024-07-25 08:34:19,015] INFO: [0;37mWarning: module GELU is treated as a zero-op.[0m
[2024-07-25 08:34:19,015] INFO: [0;37mWarning: module PairEmbed is treated as a zero-op.[0m
[2024-07-25 08:34:19,015] INFO: [0;37mWarning: module NonDynamicallyQuantizableLinear is treated as a zero-op.[0m
[2024-07-25 08:34:19,015] INFO: [0;37mWarning: module Dropout is treated as a zero-op.[0m
[2024-07-25 08:34:19,015] INFO: [0;37mWarning: module Block is treated as a zero-op.[0m
[2024-07-25 08:34:19,016] INFO: [0;37mWarning: module ParticleTransformer is treated as a zero-op.[0m
[2024-07-25 08:34:19,016] INFO: [0;37mWarning: module ParticleTransformerWrapper is treated as a zero-op.[0m
[2024-07-25 08:34:19,364] INFO: [0;37mParticleTransformerWrapper(
  |2.12 M, 99.021% Params, 341.986 MMac, 100.000% MACs|
  (mod): ParticleTransformer(
    |2.12 M, 99.021% Params, 341.986 MMac, 100.000% MACs|
    (trimmer): SequenceTrimmer(|0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
    (embed): Embed(
      |0.132 M, 6.189% Params, 16.959 MMac, 4.959% MACs|
      (input_bn): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.000% Params, 1.28 KMac, 0.000% MACs|)
      (embed): Sequential(
        |0.132 M, 6.189% Params, 16.958 MMac, 4.959% MACs|
        (0): LayerNorm((5,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (1): Linear(in_features=5, out_features=128, bias=True, |0.001 M, 0.036% Params, 82.048 KMac, 0.024% MACs|)
        (2): ReLU(|0.0 M, 0.000% Params, 16.384 KMac, 0.005% MACs|)
        (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (4): Linear(in_features=128, out_features=512, bias=True, |0.066 M, 3.085% Params, 8.389 MMac, 2.453% MACs|)
        (5): ReLU(|0.0 M, 0.000% Params, 65.536 KMac, 0.019% MACs|)
        (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (7): Linear(in_features=512, out_features=128, bias=True, |0.066 M, 3.067% Params, 8.389 MMac, 2.453% MACs|)
        (8): ReLU(|0.0 M, 0.000% Params, 16.384 KMac, 0.005% MACs|)
      )
    )
    (pair_embed): PairEmbed(
      |0.01 M, 0.447% Params, 78.993 MMac, 23.098% MACs|
      (embed): Sequential(
        |0.01 M, 0.447% Params, 78.993 MMac, 23.098% MACs|
        (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.000% Params, 66.048 KMac, 0.019% MACs|)
        (1): Conv1d(4, 64, kernel_size=(1,), stride=(1,), |0.0 M, 0.015% Params, 2.642 MMac, 0.773% MACs|)
        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.006% Params, 1.057 MMac, 0.309% MACs|)
        (3): GELU(approximate='none', |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (4): Conv1d(64, 64, kernel_size=(1,), stride=(1,), |0.004 M, 0.194% Params, 34.345 MMac, 10.043% MACs|)
        (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.006% Params, 1.057 MMac, 0.309% MACs|)
        (6): GELU(approximate='none', |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (7): Conv1d(64, 64, kernel_size=(1,), stride=(1,), |0.004 M, 0.194% Params, 34.345 MMac, 10.043% MACs|)
        (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.006% Params, 1.057 MMac, 0.309% MACs|)
        (9): GELU(approximate='none', |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (10): Conv1d(64, 8, kernel_size=(1,), stride=(1,), |0.001 M, 0.024% Params, 4.293 MMac, 1.255% MACs|)
        (11): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.001% Params, 132.096 KMac, 0.039% MACs|)
        (12): GELU(approximate='none', |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
      )
    )
    (blocks): ModuleList(
      |1.582 M, 73.903% Params, 237.114 MMac, 69.334% MACs|
      (0): Block(
        |0.198 M, 9.238% Params, 29.639 MMac, 8.667% MACs|
        (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (attn): MultiheadAttention(
          |0.066 M, 3.085% Params, 12.796 MMac, 3.742% MACs|
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        )
        (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (dropout): Dropout(p=0.1, inplace=False, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (fc1): Linear(in_features=128, out_features=512, bias=True, |0.066 M, 3.085% Params, 8.389 MMac, 2.453% MACs|)
        (act): ReLU(|0.0 M, 0.000% Params, 65.536 KMac, 0.019% MACs|)
        (act_dropout): Dropout(p=0.1, inplace=False, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (fc2): Linear(in_features=512, out_features=128, bias=True, |0.066 M, 3.067% Params, 8.389 MMac, 2.453% MACs|)
      )
      (1): Block(
        |0.198 M, 9.238% Params, 29.639 MMac, 8.667% MACs|
        (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (attn): MultiheadAttention(
          |0.066 M, 3.085% Params, 12.796 MMac, 3.742% MACs|
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        )
        (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (dropout): Dropout(p=0.1, inplace=False, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (fc1): Linear(in_features=128, out_features=512, bias=True, |0.066 M, 3.085% Params, 8.389 MMac, 2.453% MACs|)
        (act): ReLU(|0.0 M, 0.000% Params, 65.536 KMac, 0.019% MACs|)
        (act_dropout): Dropout(p=0.1, inplace=False, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (fc2): Linear(in_features=512, out_features=128, bias=True, |0.066 M, 3.067% Params, 8.389 MMac, 2.453% MACs|)
      )
      (2): Block(
        |0.198 M, 9.238% Params, 29.639 MMac, 8.667% MACs|
        (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (attn): MultiheadAttention(
          |0.066 M, 3.085% Params, 12.796 MMac, 3.742% MACs|
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        )
        (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (dropout): Dropout(p=0.1, inplace=False, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (fc1): Linear(in_features=128, out_features=512, bias=True, |0.066 M, 3.085% Params, 8.389 MMac, 2.453% MACs|)
        (act): ReLU(|0.0 M, 0.000% Params, 65.536 KMac, 0.019% MACs|)
        (act_dropout): Dropout(p=0.1, inplace=False, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (fc2): Linear(in_features=512, out_features=128, bias=True, |0.066 M, 3.067% Params, 8.389 MMac, 2.453% MACs|)
      )
      (3): Block(
        |0.198 M, 9.238% Params, 29.639 MMac, 8.667% MACs|
        (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (attn): MultiheadAttention(
          |0.066 M, 3.085% Params, 12.796 MMac, 3.742% MACs|
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        )
        (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (dropout): Dropout(p=0.1, inplace=False, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (fc1): Linear(in_features=128, out_features=512, bias=True, |0.066 M, 3.085% Params, 8.389 MMac, 2.453% MACs|)
        (act): ReLU(|0.0 M, 0.000% Params, 65.536 KMac, 0.019% MACs|)
        (act_dropout): Dropout(p=0.1, inplace=False, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (fc2): Linear(in_features=512, out_features=128, bias=True, |0.066 M, 3.067% Params, 8.389 MMac, 2.453% MACs|)
      )
      (4): Block(
        |0.198 M, 9.238% Params, 29.639 MMac, 8.667% MACs|
        (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (attn): MultiheadAttention(
          |0.066 M, 3.085% Params, 12.796 MMac, 3.742% MACs|
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        )
        (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (dropout): Dropout(p=0.1, inplace=False, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (fc1): Linear(in_features=128, out_features=512, bias=True, |0.066 M, 3.085% Params, 8.389 MMac, 2.453% MACs|)
        (act): ReLU(|0.0 M, 0.000% Params, 65.536 KMac, 0.019% MACs|)
        (act_dropout): Dropout(p=0.1, inplace=False, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (fc2): Linear(in_features=512, out_features=128, bias=True, |0.066 M, 3.067% Params, 8.389 MMac, 2.453% MACs|)
      )
      (5): Block(
        |0.198 M, 9.238% Params, 29.639 MMac, 8.667% MACs|
        (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (attn): MultiheadAttention(
          |0.066 M, 3.085% Params, 12.796 MMac, 3.742% MACs|
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        )
        (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (dropout): Dropout(p=0.1, inplace=False, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (fc1): Linear(in_features=128, out_features=512, bias=True, |0.066 M, 3.085% Params, 8.389 MMac, 2.453% MACs|)
        (act): ReLU(|0.0 M, 0.000% Params, 65.536 KMac, 0.019% MACs|)
        (act_dropout): Dropout(p=0.1, inplace=False, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (fc2): Linear(in_features=512, out_features=128, bias=True, |0.066 M, 3.067% Params, 8.389 MMac, 2.453% MACs|)
      )
      (6): Block(
        |0.198 M, 9.238% Params, 29.639 MMac, 8.667% MACs|
        (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (attn): MultiheadAttention(
          |0.066 M, 3.085% Params, 12.796 MMac, 3.742% MACs|
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        )
        (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (dropout): Dropout(p=0.1, inplace=False, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (fc1): Linear(in_features=128, out_features=512, bias=True, |0.066 M, 3.085% Params, 8.389 MMac, 2.453% MACs|)
        (act): ReLU(|0.0 M, 0.000% Params, 65.536 KMac, 0.019% MACs|)
        (act_dropout): Dropout(p=0.1, inplace=False, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (fc2): Linear(in_features=512, out_features=128, bias=True, |0.066 M, 3.067% Params, 8.389 MMac, 2.453% MACs|)
      )
      (7): Block(
        |0.198 M, 9.238% Params, 29.639 MMac, 8.667% MACs|
        (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (attn): MultiheadAttention(
          |0.066 M, 3.085% Params, 12.796 MMac, 3.742% MACs|
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        )
        (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (dropout): Dropout(p=0.1, inplace=False, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (fc1): Linear(in_features=128, out_features=512, bias=True, |0.066 M, 3.085% Params, 8.389 MMac, 2.453% MACs|)
        (act): ReLU(|0.0 M, 0.000% Params, 65.536 KMac, 0.019% MACs|)
        (act_dropout): Dropout(p=0.1, inplace=False, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (fc2): Linear(in_features=512, out_features=128, bias=True, |0.066 M, 3.067% Params, 8.389 MMac, 2.453% MACs|)
      )
    )
    (cls_blocks): ModuleList(
      |0.396 M, 18.476% Params, 8.919 MMac, 2.608% MACs|
      (0): Block(
        |0.198 M, 9.238% Params, 4.46 MMac, 1.304% MACs|
        (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (attn): MultiheadAttention(
          |0.066 M, 3.085% Params, 4.327 MMac, 1.265% MACs|
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        )
        (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (dropout): Dropout(p=0, inplace=False, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (fc1): Linear(in_features=128, out_features=512, bias=True, |0.066 M, 3.085% Params, 66.048 KMac, 0.019% MACs|)
        (act): ReLU(|0.0 M, 0.000% Params, 512.0 Mac, 0.000% MACs|)
        (act_dropout): Dropout(p=0, inplace=False, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (fc2): Linear(in_features=512, out_features=128, bias=True, |0.066 M, 3.067% Params, 65.664 KMac, 0.019% MACs|)
      )
      (1): Block(
        |0.198 M, 9.238% Params, 4.46 MMac, 1.304% MACs|
        (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (attn): MultiheadAttention(
          |0.066 M, 3.085% Params, 4.327 MMac, 1.265% MACs|
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        )
        (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (dropout): Dropout(p=0, inplace=False, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (fc1): Linear(in_features=128, out_features=512, bias=True, |0.066 M, 3.085% Params, 66.048 KMac, 0.019% MACs|)
        (act): ReLU(|0.0 M, 0.000% Params, 512.0 Mac, 0.000% MACs|)
        (act_dropout): Dropout(p=0, inplace=False, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
        (fc2): Linear(in_features=512, out_features=128, bias=True, |0.066 M, 3.067% Params, 65.664 KMac, 0.019% MACs|)
      )
    )
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True, |0.0 M, 0.000% Params, 0.0 Mac, 0.000% MACs|)
    (fc): Sequential(
      |0.0 M, 0.006% Params, 129.0 Mac, 0.000% MACs|
      (0): Linear(in_features=128, out_features=1, bias=True, |0.0 M, 0.006% Params, 129.0 Mac, 0.000% MACs|)
    )
  )
)[0m
[2024-07-25 08:34:19,365] INFO: Computational complexity:       341.99 MMac
[2024-07-25 08:34:19,366] INFO: Number of parameters:           2.14 M  
[2024-07-25 08:34:19,366] INFO: Using loss function MSELoss() with options {'use_amp': True}
[2024-07-25 08:34:19,417] INFO: Create Tensorboard summary writer with comment Bmeson_kin_ParT
[2024-07-25 08:34:19,420] INFO: Optimizer options: {'weight_decay': 0.01}
[2024-07-25 08:34:19,421] INFO: Parameters excluded from weight decay:
 - mod.cls_token
 - mod.embed.input_bn.weight
 - mod.embed.input_bn.bias
 - mod.embed.embed.0.weight
 - mod.embed.embed.0.bias
 - mod.embed.embed.1.bias
 - mod.embed.embed.3.weight
 - mod.embed.embed.3.bias
 - mod.embed.embed.4.bias
 - mod.embed.embed.6.weight
 - mod.embed.embed.6.bias
 - mod.embed.embed.7.bias
 - mod.pair_embed.embed.0.weight
 - mod.pair_embed.embed.0.bias
 - mod.pair_embed.embed.1.bias
 - mod.pair_embed.embed.2.weight
 - mod.pair_embed.embed.2.bias
 - mod.pair_embed.embed.4.bias
 - mod.pair_embed.embed.5.weight
 - mod.pair_embed.embed.5.bias
 - mod.pair_embed.embed.7.bias
 - mod.pair_embed.embed.8.weight
 - mod.pair_embed.embed.8.bias
 - mod.pair_embed.embed.10.bias
 - mod.pair_embed.embed.11.weight
 - mod.pair_embed.embed.11.bias
 - mod.blocks.0.c_attn
 - mod.blocks.0.w_resid
 - mod.blocks.0.pre_attn_norm.weight
 - mod.blocks.0.pre_attn_norm.bias
 - mod.blocks.0.attn.in_proj_bias
 - mod.blocks.0.attn.out_proj.bias
 - mod.blocks.0.post_attn_norm.weight
 - mod.blocks.0.post_attn_norm.bias
 - mod.blocks.0.pre_fc_norm.weight
 - mod.blocks.0.pre_fc_norm.bias
 - mod.blocks.0.fc1.bias
 - mod.blocks.0.post_fc_norm.weight
 - mod.blocks.0.post_fc_norm.bias
 - mod.blocks.0.fc2.bias
 - mod.blocks.1.c_attn
 - mod.blocks.1.w_resid
 - mod.blocks.1.pre_attn_norm.weight
 - mod.blocks.1.pre_attn_norm.bias
 - mod.blocks.1.attn.in_proj_bias
 - mod.blocks.1.attn.out_proj.bias
 - mod.blocks.1.post_attn_norm.weight
 - mod.blocks.1.post_attn_norm.bias
 - mod.blocks.1.pre_fc_norm.weight
 - mod.blocks.1.pre_fc_norm.bias
 - mod.blocks.1.fc1.bias
 - mod.blocks.1.post_fc_norm.weight
 - mod.blocks.1.post_fc_norm.bias
 - mod.blocks.1.fc2.bias
 - mod.blocks.2.c_attn
 - mod.blocks.2.w_resid
 - mod.blocks.2.pre_attn_norm.weight
 - mod.blocks.2.pre_attn_norm.bias
 - mod.blocks.2.attn.in_proj_bias
 - mod.blocks.2.attn.out_proj.bias
 - mod.blocks.2.post_attn_norm.weight
 - mod.blocks.2.post_attn_norm.bias
 - mod.blocks.2.pre_fc_norm.weight
 - mod.blocks.2.pre_fc_norm.bias
 - mod.blocks.2.fc1.bias
 - mod.blocks.2.post_fc_norm.weight
 - mod.blocks.2.post_fc_norm.bias
 - mod.blocks.2.fc2.bias
 - mod.blocks.3.c_attn
 - mod.blocks.3.w_resid
 - mod.blocks.3.pre_attn_norm.weight
 - mod.blocks.3.pre_attn_norm.bias
 - mod.blocks.3.attn.in_proj_bias
 - mod.blocks.3.attn.out_proj.bias
 - mod.blocks.3.post_attn_norm.weight
 - mod.blocks.3.post_attn_norm.bias
 - mod.blocks.3.pre_fc_norm.weight
 - mod.blocks.3.pre_fc_norm.bias
 - mod.blocks.3.fc1.bias
 - mod.blocks.3.post_fc_norm.weight
 - mod.blocks.3.post_fc_norm.bias
 - mod.blocks.3.fc2.bias
 - mod.blocks.4.c_attn
 - mod.blocks.4.w_resid
 - mod.blocks.4.pre_attn_norm.weight
 - mod.blocks.4.pre_attn_norm.bias
 - mod.blocks.4.attn.in_proj_bias
 - mod.blocks.4.attn.out_proj.bias
 - mod.blocks.4.post_attn_norm.weight
 - mod.blocks.4.post_attn_norm.bias
 - mod.blocks.4.pre_fc_norm.weight
 - mod.blocks.4.pre_fc_norm.bias
 - mod.blocks.4.fc1.bias
 - mod.blocks.4.post_fc_norm.weight
 - mod.blocks.4.post_fc_norm.bias
 - mod.blocks.4.fc2.bias
 - mod.blocks.5.c_attn
 - mod.blocks.5.w_resid
 - mod.blocks.5.pre_attn_norm.weight
 - mod.blocks.5.pre_attn_norm.bias
 - mod.blocks.5.attn.in_proj_bias
 - mod.blocks.5.attn.out_proj.bias
 - mod.blocks.5.post_attn_norm.weight
 - mod.blocks.5.post_attn_norm.bias
 - mod.blocks.5.pre_fc_norm.weight
 - mod.blocks.5.pre_fc_norm.bias
 - mod.blocks.5.fc1.bias
 - mod.blocks.5.post_fc_norm.weight
 - mod.blocks.5.post_fc_norm.bias
 - mod.blocks.5.fc2.bias
 - mod.blocks.6.c_attn
 - mod.blocks.6.w_resid
 - mod.blocks.6.pre_attn_norm.weight
 - mod.blocks.6.pre_attn_norm.bias
 - mod.blocks.6.attn.in_proj_bias
 - mod.blocks.6.attn.out_proj.bias
 - mod.blocks.6.post_attn_norm.weight
 - mod.blocks.6.post_attn_norm.bias
 - mod.blocks.6.pre_fc_norm.weight
 - mod.blocks.6.pre_fc_norm.bias
 - mod.blocks.6.fc1.bias
 - mod.blocks.6.post_fc_norm.weight
 - mod.blocks.6.post_fc_norm.bias
 - mod.blocks.6.fc2.bias
 - mod.blocks.7.c_attn
 - mod.blocks.7.w_resid
 - mod.blocks.7.pre_attn_norm.weight
 - mod.blocks.7.pre_attn_norm.bias
 - mod.blocks.7.attn.in_proj_bias
 - mod.blocks.7.attn.out_proj.bias
 - mod.blocks.7.post_attn_norm.weight
 - mod.blocks.7.post_attn_norm.bias
 - mod.blocks.7.pre_fc_norm.weight
 - mod.blocks.7.pre_fc_norm.bias
 - mod.blocks.7.fc1.bias
 - mod.blocks.7.post_fc_norm.weight
 - mod.blocks.7.post_fc_norm.bias
 - mod.blocks.7.fc2.bias
 - mod.cls_blocks.0.c_attn
 - mod.cls_blocks.0.w_resid
 - mod.cls_blocks.0.pre_attn_norm.weight
 - mod.cls_blocks.0.pre_attn_norm.bias
 - mod.cls_blocks.0.attn.in_proj_bias
 - mod.cls_blocks.0.attn.out_proj.bias
 - mod.cls_blocks.0.post_attn_norm.weight
 - mod.cls_blocks.0.post_attn_norm.bias
 - mod.cls_blocks.0.pre_fc_norm.weight
 - mod.cls_blocks.0.pre_fc_norm.bias
 - mod.cls_blocks.0.fc1.bias
 - mod.cls_blocks.0.post_fc_norm.weight
 - mod.cls_blocks.0.post_fc_norm.bias
 - mod.cls_blocks.0.fc2.bias
 - mod.cls_blocks.1.c_attn
 - mod.cls_blocks.1.w_resid
 - mod.cls_blocks.1.pre_attn_norm.weight
 - mod.cls_blocks.1.pre_attn_norm.bias
 - mod.cls_blocks.1.attn.in_proj_bias
 - mod.cls_blocks.1.attn.out_proj.bias
 - mod.cls_blocks.1.post_attn_norm.weight
 - mod.cls_blocks.1.post_attn_norm.bias
 - mod.cls_blocks.1.pre_fc_norm.weight
 - mod.cls_blocks.1.pre_fc_norm.bias
 - mod.cls_blocks.1.fc1.bias
 - mod.cls_blocks.1.post_fc_norm.weight
 - mod.cls_blocks.1.post_fc_norm.bias
 - mod.cls_blocks.1.fc2.bias
 - mod.norm.weight
 - mod.norm.bias
 - mod.fc.0.bias
[2024-07-25 08:34:19,423] INFO: --------------------------------------------------
[2024-07-25 08:34:19,423] INFO: Epoch #0 training
[2024-07-25 08:34:19,471] INFO: Restarted DataIter train_worker0, load_range=(0.0, 0.8889), file_list:
{
  "_": [
    "data/Bmeson/train_file.parquet"
  ]
}
[2024-07-25 08:39:11,781] INFO: Processed 1600000 entries in total (avg. speed 5472.7 entries/s)
[2024-07-25 08:39:11,782] INFO: Train AvgLoss: 0.31531, AvgMSE: 0.31531, AvgMAE: 0.12077
[2024-07-25 08:39:11,819] INFO: Epoch #0 validating
[2024-07-25 08:39:11,843] INFO: Restarted DataIter val_worker0, load_range=(0.8889, 1.0), file_list:
{
  "_": [
    "data/Bmeson/train_file.parquet"
  ]
}
[2024-07-25 08:39:21,178] INFO: Processed 199936 entries in total (avg. speed 21363.2 entries/s)
[2024-07-25 08:39:21,191] INFO: Evaluation metrics: 
    - mean_squared_error: 
3.689485585626427e-05
    - mean_absolute_error: 
0.005749126545494525
    - median_absolute_error: 
0.005826250000000144
    - mean_gamma_deviance: 
1.3214396148556613e-06
[2024-07-25 08:39:21,195] INFO: [1mEpoch #0: Current validation metric: 0.00004 (best: 0.00004)[0m
[2024-07-25 08:39:21,247] INFO: [0;37mpreprocess config: {'method': 'manual', 'data_fraction': 0.5, 'params': None}[0m
[2024-07-25 08:39:21,247] INFO: [0;37mselection: None[0m
[2024-07-25 08:39:21,247] INFO: [0;37mtest_time_selection: None[0m
[2024-07-25 08:39:21,247] INFO: [0;37mvar_funcs:
 - ('part_mask', 'ak.ones_like(part_px)')
 - ('jet_isQ', 'label')
 - ('jet_isG', '1-label')
 - ('mass', 'label')
 - ('truth_label', 'mass')[0m
[2024-07-25 08:39:21,247] INFO: [0;37minput_names: ('pf_points', 'pf_features', 'pf_vectors', 'pf_mask')[0m
[2024-07-25 08:39:21,247] INFO: [0;37minput_dicts:
 - ('pf_points', ['part_px', 'part_py', 'part_pz'])
 - ('pf_features', ['part_px', 'part_py', 'part_pz', 'part_energy', 'part_charge'])
 - ('pf_vectors', ['part_px', 'part_py', 'part_pz', 'part_energy'])
 - ('pf_mask', ['part_mask'])[0m
[2024-07-25 08:39:21,247] INFO: [0;37minput_shapes:
 - ('pf_points', (-1, 3, 128))
 - ('pf_features', (-1, 5, 128))
 - ('pf_vectors', (-1, 4, 128))
 - ('pf_mask', (-1, 1, 128))[0m
[2024-07-25 08:39:21,248] INFO: [0;37mpreprocess_params:
 - ('part_px', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_py', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_pz', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_energy', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_charge', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_mask', {'length': 128, 'pad_mode': 'constant', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})[0m
[2024-07-25 08:39:21,248] INFO: [0;37mlabel_names: ('truth_label',)[0m
[2024-07-25 08:39:21,248] INFO: [0;37mobserver_names: ()[0m
[2024-07-25 08:39:21,248] INFO: [0;37mmonitor_variables: ()[0m
[2024-07-25 08:39:21,252] INFO: [0;37mpreprocess config: {'method': 'manual', 'data_fraction': 0.5, 'params': None}[0m
[2024-07-25 08:39:21,252] INFO: [0;37mselection: None[0m
[2024-07-25 08:39:21,252] INFO: [0;37mtest_time_selection: None[0m
[2024-07-25 08:39:21,252] INFO: [0;37mvar_funcs:
 - ('part_mask', 'ak.ones_like(part_px)')
 - ('jet_isQ', 'label')
 - ('jet_isG', '1-label')
 - ('mass', 'label')
 - ('truth_label', 'mass')[0m
[2024-07-25 08:39:21,252] INFO: [0;37minput_names: ('pf_points', 'pf_features', 'pf_vectors', 'pf_mask')[0m
[2024-07-25 08:39:21,252] INFO: [0;37minput_dicts:
 - ('pf_points', ['part_px', 'part_py', 'part_pz'])
 - ('pf_features', ['part_px', 'part_py', 'part_pz', 'part_energy', 'part_charge'])
 - ('pf_vectors', ['part_px', 'part_py', 'part_pz', 'part_energy'])
 - ('pf_mask', ['part_mask'])[0m
[2024-07-25 08:39:21,252] INFO: [0;37minput_shapes:
 - ('pf_points', (-1, 3, 128))
 - ('pf_features', (-1, 5, 128))
 - ('pf_vectors', (-1, 4, 128))
 - ('pf_mask', (-1, 1, 128))[0m
[2024-07-25 08:39:21,252] INFO: [0;37mpreprocess_params:
 - ('part_px', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_py', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_pz', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_energy', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_charge', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_mask', {'length': 128, 'pad_mode': 'constant', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})[0m
[2024-07-25 08:39:21,252] INFO: [0;37mlabel_names: ('truth_label',)[0m
[2024-07-25 08:39:21,252] INFO: [0;37mobserver_names: ()[0m
[2024-07-25 08:39:21,252] INFO: [0;37mmonitor_variables: ()[0m
[2024-07-25 08:39:21,254] INFO: Loading model training/Bmeson/ParT/20240725-083417_example_ParticleTransformer_ranger_lr0.001_batch128/net_best_epoch_state.pt for eval
[2024-07-25 08:39:21,268] INFO: Running on test file group  with 1 files:
...data/Bmeson/test_file.parquet
[2024-07-25 08:39:21,273] INFO: [0;37mpreprocess config: {'method': 'manual', 'data_fraction': 0.5, 'params': None}[0m
[2024-07-25 08:39:21,273] INFO: [0;37mselection: None[0m
[2024-07-25 08:39:21,273] INFO: [0;37mtest_time_selection: None[0m
[2024-07-25 08:39:21,273] INFO: [0;37mvar_funcs:
 - ('part_mask', 'ak.ones_like(part_px)')
 - ('jet_isQ', 'label')
 - ('jet_isG', '1-label')
 - ('mass', 'label')
 - ('truth_label', 'mass')[0m
[2024-07-25 08:39:21,273] INFO: [0;37minput_names: ('pf_points', 'pf_features', 'pf_vectors', 'pf_mask')[0m
[2024-07-25 08:39:21,273] INFO: [0;37minput_dicts:
 - ('pf_points', ['part_px', 'part_py', 'part_pz'])
 - ('pf_features', ['part_px', 'part_py', 'part_pz', 'part_energy', 'part_charge'])
 - ('pf_vectors', ['part_px', 'part_py', 'part_pz', 'part_energy'])
 - ('pf_mask', ['part_mask'])[0m
[2024-07-25 08:39:21,273] INFO: [0;37minput_shapes:
 - ('pf_points', (-1, 3, 128))
 - ('pf_features', (-1, 5, 128))
 - ('pf_vectors', (-1, 4, 128))
 - ('pf_mask', (-1, 1, 128))[0m
[2024-07-25 08:39:21,273] INFO: [0;37mpreprocess_params:
 - ('part_px', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_py', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_pz', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_energy', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_charge', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_mask', {'length': 128, 'pad_mode': 'constant', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})[0m
[2024-07-25 08:39:21,273] INFO: [0;37mlabel_names: ('truth_label',)[0m
[2024-07-25 08:39:21,273] INFO: [0;37mobserver_names: ()[0m
[2024-07-25 08:39:21,273] INFO: [0;37mmonitor_variables: ()[0m
[2024-07-25 08:39:21,277] INFO: [0;37mpreprocess config: {'method': 'manual', 'data_fraction': 0.5, 'params': None}[0m
[2024-07-25 08:39:21,277] INFO: [0;37mselection: None[0m
[2024-07-25 08:39:21,277] INFO: [0;37mtest_time_selection: None[0m
[2024-07-25 08:39:21,277] INFO: [0;37mvar_funcs:
 - ('part_mask', 'ak.ones_like(part_px)')
 - ('jet_isQ', 'label')
 - ('jet_isG', '1-label')
 - ('mass', 'label')
 - ('truth_label', 'mass')[0m
[2024-07-25 08:39:21,277] INFO: [0;37minput_names: ('pf_points', 'pf_features', 'pf_vectors', 'pf_mask')[0m
[2024-07-25 08:39:21,277] INFO: [0;37minput_dicts:
 - ('pf_points', ['part_px', 'part_py', 'part_pz'])
 - ('pf_features', ['part_px', 'part_py', 'part_pz', 'part_energy', 'part_charge'])
 - ('pf_vectors', ['part_px', 'part_py', 'part_pz', 'part_energy'])
 - ('pf_mask', ['part_mask'])[0m
[2024-07-25 08:39:21,277] INFO: [0;37minput_shapes:
 - ('pf_points', (-1, 3, 128))
 - ('pf_features', (-1, 5, 128))
 - ('pf_vectors', (-1, 4, 128))
 - ('pf_mask', (-1, 1, 128))[0m
[2024-07-25 08:39:21,277] INFO: [0;37mpreprocess_params:
 - ('part_px', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_py', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_pz', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_energy', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_charge', {'length': 128, 'pad_mode': 'wrap', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})
 - ('part_mask', {'length': 128, 'pad_mode': 'constant', 'center': None, 'scale': 1, 'min': -5, 'max': 5, 'pad_value': 0})[0m
[2024-07-25 08:39:21,277] INFO: [0;37mlabel_names: ('truth_label',)[0m
[2024-07-25 08:39:21,277] INFO: [0;37mobserver_names: ()[0m
[2024-07-25 08:39:21,277] INFO: [0;37mmonitor_variables: ()[0m
[2024-07-25 08:39:21,297] INFO: Restarted DataIter test__worker0, load_range=(0, 1), file_list:
{
  "": [
    "data/Bmeson/test_file.parquet"
  ]
}
[2024-07-25 08:39:21,824] INFO: Processed 7023 entries in total (avg. speed 12858.0 entries/s)
[2024-07-25 08:39:21,825] INFO: Evaluation metrics: 
    - mean_squared_error: 
3.679671679948879e-05
    - mean_absolute_error: 
0.00574615602306703
    - median_absolute_error: 
0.005826250000000144
    - mean_gamma_deviance: 
1.3179303816983277e-06
[2024-07-25 08:39:21,826] INFO: [1mTest metric 0.00000[0m
[2024-07-25 08:39:21,958] INFO: [1mWritten output to training/Bmeson/ParT/20240725-083417_example_ParticleTransformer_ranger_lr0.001_batch128/predict_output/pred.root[0m
